# -*- coding: utf-8 -*-
"""A1_BA820.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j9JBzXui12VSlHw_j39O7u05p8QCVDH9
"""

! pip install scikit-plot
#imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import fcluster
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from sklearn import metrics 
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics import silhouette_samples, silhouette_score

import scikitplot as skplot

# get the data set
stocks = pd.read_csv('stock-fundamentals.csv')
stocks.head(3)

stocks.describe().T

# let's take a look at the data set
stocks.info()

# after looking at the columns, I will do some testing to identify the necessary columns
a = stocks['Shares'] == stocks['Shares split adjusted']
a.values.sum() == 755

# checking for duplicates in Shares
b = list(stocks.Shares.unique())
len(b)

# drop the unnessary columns, I identified in the step ahead
stocks.drop(columns=['Quarter end', 'ticker', 'quarter_end', 'Split factor','Shares split adjusted'], inplace=True)

# replace missing values with the mean and lower case column names
stocks = stocks.fillna(stocks.mean())
stocks.columns = stocks.columns.str.lower()

print(stocks)

# double checking if there are any missing values 
print(stocks.isna().sum().sum())

# first, I am going to scale the data given the varying units of measurement
sc = StandardScaler()
sm = sc.fit_transform(stocks)

sm = pd.DataFrame(sm, columns=stocks.columns)

# confirm the changes
sm.head(3)

# Hierarchical Clustering - first attempt
# going to do euclidean, cosine, jaccard, cityblock distance
diste = pdist(sm.values)
distc = pdist(sm.values, metric="cosine")
distj = pdist(sm.values, metric="jaccard")
distm = pdist(sm.values, metric="cityblock")

# put all on the same linkage to compare
hclust_e = linkage(diste)
hclust_c = linkage(distc)
hclust_j = linkage(distj)
hclust_m = linkage(distm)

# plots
LINKS = [hclust_e, hclust_c, hclust_j,hclust_m]
TITLE = ['Euclidean', 'Cosine', 'Jaccard', 'Manhattan']

plt.figure(figsize=(15, 5))

# loop and build our plot
for i, m in enumerate(LINKS):
  plt.subplot(1, 4, i+1)
  plt.title(TITLE[i])
  dendrogram(m,
             leaf_rotation=90,
             orientation="left")
  
plt.show()

#cosine - more "clusters" visible
METHODS = ['single', 'complete', 'average', 'ward']
plt.figure(figsize=(20,5))


# loop and build our plot for the different methods
for i, m in enumerate(METHODS):
  plt.subplot(1, 4, i+1)
  plt.title(m)
  dendrogram(linkage(distc, method=m), 
             leaf_rotation=90)
  
plt.show()

# using cosine + complete
# the labels with 7 clusters
labs = fcluster(linkage(distc, method="complete"), 7, criterion="maxclust")

# confirm
np.unique(labs)

# add a cluster column to the stocks data set
stocks['cluster'] = labs
print(stocks.head(3))

#let see if the data in the cluster is evenly distributed
print(stocks.cluster.value_counts(dropna=False, sort=False))

# cluster solution
clus_profile = stocks.groupby("cluster").mean()

clus_profile.T

# heatmap plot of the clusters with normalized data
scp = StandardScaler()
cluster_scaled = scp.fit_transform(clus_profile)

cluster_scaled = pd.DataFrame(cluster_scaled, 
                              index=clus_profile.index, 
                              columns=clus_profile.columns)

sns.heatmap(cluster_scaled, cmap="Blues", center=0)
plt.show()

# findings
## It's easy to see that some clusters have higher averages in various metrics than others

# another method: KMeans 
xs = sc.fit_transform(stocks)
X = pd.DataFrame(xs, index=stocks.index, columns=stocks.columns)

# Kmeans for 2 to 8 clusters
KS = range(2, 8)

# storage
inertia = []
silo = []

for k in KS:
  km = KMeans(k)
  km.fit(X)
  labs = km.predict(X)
  inertia.append(km.inertia_)
  silo.append(silhouette_score(X, labs))

print(silo)

#plot 
plt.figure(figsize=(15,5))


plt.subplot(1, 2, 1)
plt.title("Inertia")
sns.lineplot(KS, inertia)

plt.subplot(1, 2, 2)
plt.title("Silohouette Score")
sns.lineplot(KS, silo)

plt.show()

for i, s in enumerate(silo[:10]):
  print(i+2,s)

# looks like 5 is a good point
# get the model
k5 = KMeans(5)
k5_labs = k5.fit_predict(X)

# metrics
k5_silo = silhouette_score(X, k5_labs)
k5_ssamps = silhouette_samples(X, k5_labs)
np.unique(k5_labs)

# lets compare via silo

skplot.metrics.plot_silhouette(X, labs, title="HClust", figsize=(15,5))
plt.show()

skplot.metrics.plot_silhouette(X, k5_labs, title="KMeans - 5", figsize=(15,5))
plt.show()

# both outcomes are not as nice as I hoped
# KMeans is a bit better but both perform poorly