{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Midterm_wrapup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fatgat8P41Ee"
      },
      "source": [
        "# installs\n",
        "! pip install scikit-learn\n",
        "! pip install umap-learn\n",
        "! pip install scikit-plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy9EAMKD5GiN"
      },
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "#scipy\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "#sklearn\n",
        "from sklearn import metrics \n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import scikitplot as skplot\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCUs4aEO4VKy"
      },
      "source": [
        "# auth into GCP Big Query\n",
        "\n",
        "# COLAB Only\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "# for non-Colab\n",
        "# see resources, as long as token with env var setup properly, below should work"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIzwm1DH4Y4w"
      },
      "source": [
        "# get the data\n",
        "SQL = \"SELECT * from `questrom.datasets.mtcars`\"\n",
        "YOUR_BILLING_PROJECT = \"ba-775pd\"\n",
        "\n",
        "df = pd.read_gbq(SQL, YOUR_BILLING_PROJECT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG86e7EM5yT9"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW4SXUmv4Y9R"
      },
      "source": [
        "# lets drop the model column and use it as the index\n",
        "cars.index = cars.model\n",
        "cars.drop(columns=\"model\", inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiuq10-151kx"
      },
      "source": [
        "# keep just the continous variables\n",
        "cars2 = pd.concat((cars.loc[:, \"mpg\"], cars.loc[:, \"disp\":\"qsec\"]), axis=1)\n",
        "\n",
        "# or\n",
        "# drop the unnessary columns, I identified in the step ahead\n",
        "stocks.drop(columns=['Quarter end', 'ticker', 'quarter_end', 'Split factor','Shares split adjusted'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86OozXv06Bp8"
      },
      "source": [
        "# replace missing values with the mean and lower case column names\n",
        "stocks = stocks.fillna(stocks.mean())\n",
        "stocks.columns = stocks.columns.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViodaC4K6B-o"
      },
      "source": [
        "# double checking if there are any missing values \n",
        "print(stocks.isna().sum().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zExQQ3oA6JyN"
      },
      "source": [
        "# Scaling the Data\n",
        "only if the data varies a lot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQYGfetG6E4C"
      },
      "source": [
        "# first, I am going to scale the data given the varying units of measurement\n",
        "sc = StandardScaler() #standardize features\n",
        "nl = Normalizer() #rescales each sample\n",
        "sm = sc.fit_transform(stocks)\n",
        "\n",
        "sm = pd.DataFrame(sm, columns=stocks.columns)\n",
        "\n",
        "# confirm the changes\n",
        "sm.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typnPhkKHmCg"
      },
      "source": [
        "# Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag-EtqLsHiA7"
      },
      "source": [
        "# Hierarchical Clustering - first attempt\n",
        "# going to do euclidean, cosine, jaccard, cityblock distance\n",
        "diste = pdist(sm.values)\n",
        "distc = pdist(sm.values, metric=\"cosine\")\n",
        "distj = pdist(sm.values, metric=\"jaccard\")\n",
        "distm = pdist(sm.values, metric=\"cityblock\")\n",
        "\n",
        "# put all on the same linkage to compare\n",
        "hclust_e = linkage(diste)\n",
        "hclust_c = linkage(distc)\n",
        "hclust_j = linkage(distj)\n",
        "hclust_m = linkage(distm)\n",
        "\n",
        "# plots\n",
        "LINKS = [hclust_e, hclust_c, hclust_j,hclust_m]\n",
        "TITLE = ['Euclidean', 'Cosine', 'Jaccard', 'Manhattan']\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# loop and build our plot\n",
        "for i, m in enumerate(LINKS):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.title(TITLE[i])\n",
        "  dendrogram(m,\n",
        "             leaf_rotation=90,\n",
        "             orientation=\"left\")\n",
        "  \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4mzdEqHqPq"
      },
      "source": [
        "#chose the one with the best clusterts - more \"clusters\" visible\n",
        "METHODS = ['single', 'complete', 'average', 'ward']\n",
        "plt.figure(figsize=(20,5))\n",
        "\n",
        "\n",
        "# loop and build our plot for the different methods\n",
        "for i, m in enumerate(METHODS):\n",
        "  plt.subplot(1, 4, i+1)\n",
        "  plt.title(m)\n",
        "  dendrogram(linkage(distc, method=m), \n",
        "             leaf_rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuxy7kGUH0H3"
      },
      "source": [
        "# Example: using cosine + complete\n",
        "# the labels with 7 clusters\n",
        "labs = fcluster(linkage(distc, method=\"complete\"), 7, criterion=\"maxclust\")\n",
        "\n",
        "# confirm\n",
        "np.unique(labs)\n",
        "\n",
        "# add a cluster column to the stocks data set\n",
        "stocks['cluster'] = labs\n",
        "print(stocks.head(3))\n",
        "\n",
        "#let see if the data in the cluster is evenly distributed\n",
        "print(stocks.cluster.value_counts(dropna=False, sort=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfSftKyLH81R"
      },
      "source": [
        "# cluster solution\n",
        "clus_profile = stocks.groupby(\"cluster\").mean()\n",
        "\n",
        "clus_profile.T\n",
        "\n",
        "# heatmap plot of the clusters with normalized data\n",
        "scp = StandardScaler()\n",
        "cluster_scaled = scp.fit_transform(clus_profile)\n",
        "\n",
        "cluster_scaled = pd.DataFrame(cluster_scaled, \n",
        "                              index=clus_profile.index, \n",
        "                              columns=clus_profile.columns)\n",
        "\n",
        "sns.heatmap(cluster_scaled, cmap=\"Blues\", center=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu5Out6nH9Tk"
      },
      "source": [
        "# KMeans\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZRSxLBgH_Y_"
      },
      "source": [
        "# another method: KMeans \n",
        "xs = sc.fit_transform(stocks)\n",
        "X = pd.DataFrame(xs, index=stocks.index, columns=stocks.columns)\n",
        "\n",
        "# Kmeans for 2 to 30 clusters\n",
        "KS = range(2, 30)\n",
        "\n",
        "# storage\n",
        "inertia = []\n",
        "silo = []\n",
        "\n",
        "for k in KS:\n",
        "  km = KMeans(k)\n",
        "  km.fit(X)\n",
        "  labs = km.predict(X)\n",
        "  inertia.append(km.inertia_)\n",
        "  silo.append(silhouette_score(X, labs))\n",
        "\n",
        "print(silo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eWfE9EnIBy2"
      },
      "source": [
        "#plot \n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Inertia\")\n",
        "sns.lineplot(KS, inertia)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Silohouette Score\")\n",
        "sns.lineplot(KS, silo)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DpWe6gjIDnT"
      },
      "source": [
        "for i, s in enumerate(silo[:10]):\n",
        "  print(i+2,s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkrFi43wIFaI"
      },
      "source": [
        "# looks like 5 is a good point\n",
        "# get the model\n",
        "k5 = KMeans(5)\n",
        "k5_labs = k5.fit_predict(X)\n",
        "\n",
        "# metrics\n",
        "k5_silo = silhouette_score(X, k5_labs)\n",
        "k5_ssamps = silhouette_samples(X, k5_labs)\n",
        "np.unique(k5_labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut17Gc8QIHw7"
      },
      "source": [
        "# lets compare via silo\n",
        "\n",
        "skplot.metrics.plot_silhouette(X, labs, title=\"HClust\", figsize=(15,5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufw-hp8yIJXr"
      },
      "source": [
        "skplot.metrics.plot_silhouette(X, k5_labs, title=\"KMeans - 5\", figsize=(15,5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8S5P-4IRo0"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYVf4H0nITgg"
      },
      "source": [
        "# Let try another method -PCA\n",
        "\n",
        "pca = PCA()\n",
        "pcs = pca.fit_transform(stocks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf9QdLNIuIS"
      },
      "source": [
        "# shape confirmation (rows/features) are identical SHAPES\n",
        "pcs.shape == stocks.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu4MxbI-JdSg"
      },
      "source": [
        "# first, lets get the explained variance\n",
        "# elbow plot\n",
        "\n",
        "varexp = pca.explained_variance_ratio_\n",
        "sns.lineplot(range(1, len(varexp)+1), varexp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq2VDAuQJdVe"
      },
      "source": [
        "# cumulative variance\n",
        "\n",
        "plt.title(\"Cumulative Explained Variance\")\n",
        "plt.plot(range(1, len(varexp)+1), np.cumsum(varexp))\n",
        "plt.axhline(.987)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mfrTrFoNIok"
      },
      "source": [
        "# quick function to construct the barplot easily\n",
        "def ev_plot(ev):\n",
        "  y = list(ev)\n",
        "  x = list(range(1,len(ev)+1))\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG1rwDwPNKhE"
      },
      "source": [
        "# another approach for selection is to use explained variance > 1\n",
        "\n",
        "x, y = ev_plot(pca.explained_variance_)\n",
        "sns.barplot(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDPLf863NKje"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYdAniAgNKmf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}